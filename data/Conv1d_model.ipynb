{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f28cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a6c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"database_mscc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417407a1-8113-4ccb-afb1-1b895172d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_for_model[0] = df_for_model[0].str.replace('[','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937e02c6-558b-4185-b22f-a72f23c79caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>female_surprised</td>\n",
       "      <td>-29.824322</td>\n",
       "      <td>-30.296280</td>\n",
       "      <td>-30.341100</td>\n",
       "      <td>-30.838333</td>\n",
       "      <td>-31.067204</td>\n",
       "      <td>-30.690607</td>\n",
       "      <td>-29.505543</td>\n",
       "      <td>-30.370670</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female_fearful</td>\n",
       "      <td>-31.363651</td>\n",
       "      <td>-31.014317</td>\n",
       "      <td>-31.938639</td>\n",
       "      <td>-33.056618</td>\n",
       "      <td>-30.624622</td>\n",
       "      <td>-29.960500</td>\n",
       "      <td>-32.259970</td>\n",
       "      <td>-34.145702</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female_disgust</td>\n",
       "      <td>-31.545818</td>\n",
       "      <td>-31.020504</td>\n",
       "      <td>-30.576956</td>\n",
       "      <td>-31.558160</td>\n",
       "      <td>-31.198029</td>\n",
       "      <td>-30.364178</td>\n",
       "      <td>-29.075739</td>\n",
       "      <td>-30.090372</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>female_surprised</td>\n",
       "      <td>-34.585396</td>\n",
       "      <td>-31.664700</td>\n",
       "      <td>-30.524185</td>\n",
       "      <td>-30.788443</td>\n",
       "      <td>-31.743572</td>\n",
       "      <td>-32.151950</td>\n",
       "      <td>-31.750254</td>\n",
       "      <td>-30.014618</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female_disgust</td>\n",
       "      <td>-30.627995</td>\n",
       "      <td>-31.362064</td>\n",
       "      <td>-31.129383</td>\n",
       "      <td>-32.042410</td>\n",
       "      <td>-33.471330</td>\n",
       "      <td>-34.445076</td>\n",
       "      <td>-33.192543</td>\n",
       "      <td>-31.554546</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 y          0          1          2          3  \\\n",
       "0           0  female_surprised -29.824322 -30.296280 -30.341100 -30.838333   \n",
       "1           1    female_fearful -31.363651 -31.014317 -31.938639 -33.056618   \n",
       "2           2    female_disgust -31.545818 -31.020504 -30.576956 -31.558160   \n",
       "3           3  female_surprised -34.585396 -31.664700 -30.524185 -30.788443   \n",
       "4           4    female_disgust -30.627995 -31.362064 -31.129383 -32.042410   \n",
       "\n",
       "           4          5          6          7  ...  219  220  221  222  223  \\\n",
       "0 -31.067204 -30.690607 -29.505543 -30.370670  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "1 -30.624622 -29.960500 -32.259970 -34.145702  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "2 -31.198029 -30.364178 -29.075739 -30.090372  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "3 -31.743572 -32.151950 -31.750254 -30.014618  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "4 -33.471330 -34.445076 -33.192543 -31.554546  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   224  225  226  227  228  \n",
       "0  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5970ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = \"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bb6521-2929-427e-af85-632db5800a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>female_disgust</td>\n",
       "      <td>-29.268341</td>\n",
       "      <td>-29.184180</td>\n",
       "      <td>-30.172647</td>\n",
       "      <td>-30.977152</td>\n",
       "      <td>-31.838312</td>\n",
       "      <td>-32.326775</td>\n",
       "      <td>-31.773617</td>\n",
       "      <td>-30.989521</td>\n",
       "      <td>-29.947556</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>-30.255762</td>\n",
       "      <td>-29.088932</td>\n",
       "      <td>-28.198680</td>\n",
       "      <td>-27.928244</td>\n",
       "      <td>-27.569020</td>\n",
       "      <td>-27.578817</td>\n",
       "      <td>-27.733551</td>\n",
       "      <td>-27.521640</td>\n",
       "      <td>-28.491644</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>female_surprised</td>\n",
       "      <td>-31.644619</td>\n",
       "      <td>-28.903180</td>\n",
       "      <td>-28.716227</td>\n",
       "      <td>-29.596594</td>\n",
       "      <td>-30.896626</td>\n",
       "      <td>-30.909430</td>\n",
       "      <td>-29.661102</td>\n",
       "      <td>-30.447230</td>\n",
       "      <td>-32.236750</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>female_surprised</td>\n",
       "      <td>-31.467510</td>\n",
       "      <td>-30.668491</td>\n",
       "      <td>-30.403183</td>\n",
       "      <td>-28.884400</td>\n",
       "      <td>-28.901764</td>\n",
       "      <td>-30.096973</td>\n",
       "      <td>-30.307621</td>\n",
       "      <td>-30.522541</td>\n",
       "      <td>-31.195232</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>female_disgust</td>\n",
       "      <td>-34.253716</td>\n",
       "      <td>-32.424217</td>\n",
       "      <td>-31.304874</td>\n",
       "      <td>-30.410060</td>\n",
       "      <td>-30.121990</td>\n",
       "      <td>-29.580973</td>\n",
       "      <td>-30.290380</td>\n",
       "      <td>-31.769410</td>\n",
       "      <td>-33.591330</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     y          0          1          2          3          4  \\\n",
       "1435    female_disgust -29.268341 -29.184180 -30.172647 -30.977152 -31.838312   \n",
       "1436    female_fearful -30.255762 -29.088932 -28.198680 -27.928244 -27.569020   \n",
       "1437  female_surprised -31.644619 -28.903180 -28.716227 -29.596594 -30.896626   \n",
       "1438  female_surprised -31.467510 -30.668491 -30.403183 -28.884400 -28.901764   \n",
       "1439    female_disgust -34.253716 -32.424217 -31.304874 -30.410060 -30.121990   \n",
       "\n",
       "              5          6          7          8  ...  219  220  221  222  \\\n",
       "1435 -32.326775 -31.773617 -30.989521 -29.947556  ...  NaN  NaN  NaN  NaN   \n",
       "1436 -27.578817 -27.733551 -27.521640 -28.491644  ...  NaN  NaN  NaN  NaN   \n",
       "1437 -30.909430 -29.661102 -30.447230 -32.236750  ...  NaN  NaN  NaN  NaN   \n",
       "1438 -30.096973 -30.307621 -30.522541 -31.195232  ...  NaN  NaN  NaN  NaN   \n",
       "1439 -29.580973 -30.290380 -31.769410 -33.591330  ...  NaN  NaN  NaN  NaN   \n",
       "\n",
       "      223  224  225  226  227  228  \n",
       "1435  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1436  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1437  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1438  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1439  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968bbde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/cjq_vxdx7vqfc6jkyf5tpx4r0000gn/T/ipykernel_8424/4108488883.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"y\"][row] = df[\"y\"][row][7:]\n"
     ]
    }
   ],
   "source": [
    "for row in np.arange(0,df.shape[0]):\n",
    "    df[\"y\"][row] = df[\"y\"][row][7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f121017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y         0\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "       ... \n",
       "224    1439\n",
       "225    1439\n",
       "226    1439\n",
       "227    1439\n",
       "228    1440\n",
       "Length: 230, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca5668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b264bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     surprised\n",
       "1       fearful\n",
       "2       disgust\n",
       "14        happy\n",
       "15         calm\n",
       "18      neutral\n",
       "34          sad\n",
       "35        angry\n",
       "Name: y, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199b76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"y\"]\n",
    "X = df.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb86c8d-22d7-48e4-9b44-602fea47eb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-29.824322</td>\n",
       "      <td>-30.296280</td>\n",
       "      <td>-30.341100</td>\n",
       "      <td>-30.838333</td>\n",
       "      <td>-31.067204</td>\n",
       "      <td>-30.690607</td>\n",
       "      <td>-29.505543</td>\n",
       "      <td>-30.370670</td>\n",
       "      <td>-30.751425</td>\n",
       "      <td>-30.751398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-31.363651</td>\n",
       "      <td>-31.014317</td>\n",
       "      <td>-31.938639</td>\n",
       "      <td>-33.056618</td>\n",
       "      <td>-30.624622</td>\n",
       "      <td>-29.960500</td>\n",
       "      <td>-32.259970</td>\n",
       "      <td>-34.145702</td>\n",
       "      <td>-34.189300</td>\n",
       "      <td>-33.045395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-31.545818</td>\n",
       "      <td>-31.020504</td>\n",
       "      <td>-30.576956</td>\n",
       "      <td>-31.558160</td>\n",
       "      <td>-31.198029</td>\n",
       "      <td>-30.364178</td>\n",
       "      <td>-29.075739</td>\n",
       "      <td>-30.090372</td>\n",
       "      <td>-29.831460</td>\n",
       "      <td>-30.690586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-34.585396</td>\n",
       "      <td>-31.664700</td>\n",
       "      <td>-30.524185</td>\n",
       "      <td>-30.788443</td>\n",
       "      <td>-31.743572</td>\n",
       "      <td>-32.151950</td>\n",
       "      <td>-31.750254</td>\n",
       "      <td>-30.014618</td>\n",
       "      <td>-30.377722</td>\n",
       "      <td>-31.105500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-30.627995</td>\n",
       "      <td>-31.362064</td>\n",
       "      <td>-31.129383</td>\n",
       "      <td>-32.042410</td>\n",
       "      <td>-33.471330</td>\n",
       "      <td>-34.445076</td>\n",
       "      <td>-33.192543</td>\n",
       "      <td>-31.554546</td>\n",
       "      <td>-31.159200</td>\n",
       "      <td>-31.269037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>-29.268341</td>\n",
       "      <td>-29.184180</td>\n",
       "      <td>-30.172647</td>\n",
       "      <td>-30.977152</td>\n",
       "      <td>-31.838312</td>\n",
       "      <td>-32.326775</td>\n",
       "      <td>-31.773617</td>\n",
       "      <td>-30.989521</td>\n",
       "      <td>-29.947556</td>\n",
       "      <td>-28.873627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>-30.255762</td>\n",
       "      <td>-29.088932</td>\n",
       "      <td>-28.198680</td>\n",
       "      <td>-27.928244</td>\n",
       "      <td>-27.569020</td>\n",
       "      <td>-27.578817</td>\n",
       "      <td>-27.733551</td>\n",
       "      <td>-27.521640</td>\n",
       "      <td>-28.491644</td>\n",
       "      <td>-29.975315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>-31.644619</td>\n",
       "      <td>-28.903180</td>\n",
       "      <td>-28.716227</td>\n",
       "      <td>-29.596594</td>\n",
       "      <td>-30.896626</td>\n",
       "      <td>-30.909430</td>\n",
       "      <td>-29.661102</td>\n",
       "      <td>-30.447230</td>\n",
       "      <td>-32.236750</td>\n",
       "      <td>-31.757404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>-31.467510</td>\n",
       "      <td>-30.668491</td>\n",
       "      <td>-30.403183</td>\n",
       "      <td>-28.884400</td>\n",
       "      <td>-28.901764</td>\n",
       "      <td>-30.096973</td>\n",
       "      <td>-30.307621</td>\n",
       "      <td>-30.522541</td>\n",
       "      <td>-31.195232</td>\n",
       "      <td>-31.903524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>-34.253716</td>\n",
       "      <td>-32.424217</td>\n",
       "      <td>-31.304874</td>\n",
       "      <td>-30.410060</td>\n",
       "      <td>-30.121990</td>\n",
       "      <td>-29.580973</td>\n",
       "      <td>-30.290380</td>\n",
       "      <td>-31.769410</td>\n",
       "      <td>-33.591330</td>\n",
       "      <td>-31.698383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows Ã— 229 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "0    -29.824322 -30.296280 -30.341100 -30.838333 -31.067204 -30.690607   \n",
       "1    -31.363651 -31.014317 -31.938639 -33.056618 -30.624622 -29.960500   \n",
       "2    -31.545818 -31.020504 -30.576956 -31.558160 -31.198029 -30.364178   \n",
       "3    -34.585396 -31.664700 -30.524185 -30.788443 -31.743572 -32.151950   \n",
       "4    -30.627995 -31.362064 -31.129383 -32.042410 -33.471330 -34.445076   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1435 -29.268341 -29.184180 -30.172647 -30.977152 -31.838312 -32.326775   \n",
       "1436 -30.255762 -29.088932 -28.198680 -27.928244 -27.569020 -27.578817   \n",
       "1437 -31.644619 -28.903180 -28.716227 -29.596594 -30.896626 -30.909430   \n",
       "1438 -31.467510 -30.668491 -30.403183 -28.884400 -28.901764 -30.096973   \n",
       "1439 -34.253716 -32.424217 -31.304874 -30.410060 -30.121990 -29.580973   \n",
       "\n",
       "              6          7          8          9  ...  219  220  221  222  \\\n",
       "0    -29.505543 -30.370670 -30.751425 -30.751398  ...  0.0  0.0  0.0  0.0   \n",
       "1    -32.259970 -34.145702 -34.189300 -33.045395  ...  0.0  0.0  0.0  0.0   \n",
       "2    -29.075739 -30.090372 -29.831460 -30.690586  ...  0.0  0.0  0.0  0.0   \n",
       "3    -31.750254 -30.014618 -30.377722 -31.105500  ...  0.0  0.0  0.0  0.0   \n",
       "4    -33.192543 -31.554546 -31.159200 -31.269037  ...  0.0  0.0  0.0  0.0   \n",
       "...         ...        ...        ...        ...  ...  ...  ...  ...  ...   \n",
       "1435 -31.773617 -30.989521 -29.947556 -28.873627  ...  0.0  0.0  0.0  0.0   \n",
       "1436 -27.733551 -27.521640 -28.491644 -29.975315  ...  0.0  0.0  0.0  0.0   \n",
       "1437 -29.661102 -30.447230 -32.236750 -31.757404  ...  0.0  0.0  0.0  0.0   \n",
       "1438 -30.307621 -30.522541 -31.195232 -31.903524  ...  0.0  0.0  0.0  0.0   \n",
       "1439 -30.290380 -31.769410 -33.591330 -31.698383  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      223  224  225  226  227  228  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  \n",
       "1435  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1436  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1437  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1438  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1439  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1440 rows x 229 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ae63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c4b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "y_train_cat = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test_cat = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c453fddd-f9ee-49ec-ade5-a1d1db3c53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c7dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe482984",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_array = np.array(X_train_scaled)\n",
    "X_test_scaled_array = np.array(X_test_scaled)\n",
    "y_train_array = np.array(y_train_cat)\n",
    "y_test_array = np.array(y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cef30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_expanded_dims = np.expand_dims(X_train_scaled_array, axis=2)\n",
    "X_test_expanded_dims = np.expand_dims(X_test_scaled_array, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "415aa053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = models.Sequential()\\nmodel.add(layers.Conv1D(256, 8, padding=\"same\", input_shape=(X_train_expanded_dims.shape[1],1), activation = \"tanh\"))\\nmodel.add(layers.Conv1D(256, 8, padding=\\'same\\', activation = \"relu\"))\\n#model.add(layers.BatchNormalization())\\nmodel.add(layers.Dropout(0.25))\\nmodel.add(layers.MaxPooling1D(pool_size=(8)))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\n#model.add(layers.BatchNormalization())\\nmodel.add(layers.Dropout(0.25))\\nmodel.add(layers.MaxPooling1D(pool_size=(8)))\\nmodel.add(layers.Conv1D(64, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(64, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(8, activation = \"softmax\"))\\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from tensorflow import keras\n",
    "model = keras.Sequential([\n",
    "\n",
    "        # input layer\n",
    "        keras.layers.Flatten(input_shape=(X_train_expanded_dims.shape[1],1)),\n",
    "\n",
    "        # 1st dense layer\n",
    "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        # 2nd dense layer\n",
    "        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        # 3rd dense layer\n",
    "        keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        # output layer\n",
    "        keras.layers.Dense(8, activation='softmax')\n",
    "    ])\"\"\"\n",
    "\n",
    "\"\"\"from tensorflow import keras\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(X_train_expanded_dims.shape[1],1)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv1D(128, 5,padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(128, 5,padding='same',))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv1D(128, 5,padding='same',))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv1D(128, 5,padding='same',))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv1D(128, 5,padding='same',))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(8))\n",
    "model.add(layers.Activation('softmax'))\n",
    "opt = keras.optimizers.Adam(lr=0.00001, decay=1e-6)\"\"\"\n",
    "\n",
    "\"\"\"model = models.Sequential()\n",
    "model.add(layers.Conv1D(256, 8, padding=\"same\", input_shape=(X_train_expanded_dims.shape[1],1), activation = \"tanh\"))\n",
    "model.add(layers.Conv1D(256, 8, padding='same', activation = \"relu\"))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(64, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(64, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(8, activation = \"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6a678bd-967d-48b7-a89c-f65a1e9e75c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras.layers import Dense, Embedding, LSTM\\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\\nfrom tensorflow import keras\\nmodel = models.Sequential()\\nmodel.add(layers.Conv1D(256, 8, padding=\\'same\\',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\\nmodel.add(Activation(\\'relu\\'))\\nmodel.add(layers.Conv1D(256, 8, padding=\\'same\\'))\\nmodel.add(layers.Activation(\\'relu\\'))\\n#model.add(layers.BatchNormalization())\\nmodel.add(layers.Dropout(0.25))\\nmodel.add(layers.MaxPooling1D(pool_size=(8)))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\n#model.add(layers.Conv1D(128, 8, padding=\\'same\\'))#\\n#model.add(layers.Conv1D(128, 8, padding=\\'same\\'))#\\n#model.add(layers.Activation(\\'relu\\'))#\\n#model.add(layers.Conv1D(128, 8, padding=\\'same\\'))#\\n#model.add(layers.Activation(\\'relu\\'))#\\n#model.add(layers.BatchNormalization())\\nmodel.add(layers.Dropout(0.25))\\nmodel.add(layers.MaxPooling1D(pool_size=(8)))\\nmodel.add(layers.Conv1D(64, 8, padding=\\'same\\'))\\nmodel.add(layers.Activation(\\'relu\\'))\\n#model.add(layers.Conv1D(64, 8, padding=\\'same\\'))\\n#model.add(layers.Activation(\\'relu\\'))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(8, activation = \"softmax\"))\\n#opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\\nopt = keras.optimizers.Adam(lr=0.0001)\\n#opt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\\nmodel.summary()'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Good model\n",
    "\"\"\"from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from tensorflow import keras\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(layers.Conv1D(256, 8, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "#model.add(layers.Conv1D(128, 8, padding='same'))#\n",
    "#model.add(layers.Conv1D(128, 8, padding='same'))#\n",
    "#model.add(layers.Activation('relu'))#\n",
    "#model.add(layers.Conv1D(128, 8, padding='same'))#\n",
    "#model.add(layers.Activation('relu'))#\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(64, 8, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "#model.add(layers.Conv1D(64, 8, padding='same'))\n",
    "#model.add(layers.Activation('relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(8, activation = \"softmax\"))\n",
    "#opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "#opt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\n",
    "model.summary()\"\"\"\n",
    "#0.36\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fab1ee5-ffd5-4cab-a6ce-ec0dd1944d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = models.Sequential()\\nmodel.add(layers.Conv1D(256, 8, padding=\"same\", input_shape=(X_train_expanded_dims.shape[1],1), activation = \"tanh\"))\\nmodel.add(layers.Conv1D(256, 8, padding=\\'same\\', activation = \"relu\"))\\n#model.add(layers.BatchNormalization())\\nmodel.add(layers.Dropout(0.1))\\nmodel.add(layers.MaxPooling1D(pool_size=(8)))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(128, 8, padding=\\'same\\', activation = \"relu\"))\\n#model.add(layers.BatchNormalization())\\nmodel.add(layers.Dropout(0.1))\\nmodel.add(layers.MaxPooling1D(pool_size=(8)))\\nmodel.add(layers.Conv1D(64, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Conv1D(64, 8, padding=\\'same\\', activation = \"relu\"))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(8, activation = \"softmax\"))\\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\\nopt = keras.optimizers.Adam(lr=0.00001, decay=1e-6)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = models.Sequential()\n",
    "model.add(layers.Conv1D(256, 8, padding=\"same\", input_shape=(X_train_expanded_dims.shape[1],1), activation = \"tanh\"))\n",
    "model.add(layers.Conv1D(256, 8, padding='same', activation = \"relu\"))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(64, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(64, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(8, activation = \"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "opt = keras.optimizers.Adam(lr=0.00001, decay=1e-6)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "845c86d0-670f-4137-9598-d9a9bb4fa96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import keras\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport tensorflow as tf\\nfrom keras.preprocessing import sequence\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Embedding\\n#from keras.utils import to_categorical\\nfrom keras.layers import Input, Flatten, Dropout, Activation\\nfrom keras.layers import Conv1D, MaxPooling1D\\nfrom keras.models import Model\\nfrom keras.callbacks import ModelCheckpoint\\nfrom tensorflow import keras\\n\\nmodel = Sequential()\\n\\nmodel.add(layers.Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\\nmodel.add(Activation('relu'))\\nmodel.add(layers.Conv1D(256, 8, padding='same'))\\nmodel.add(layers.Activation('relu'))\\n\\nmodel.add(Conv1D(128, 5,padding='same',input_shape=(X_train_expanded_dims.shape[1],1)))      #1\\nmodel.add(Activation('relu'))\\nmodel.add(Dropout(0.1))\\nmodel.add(MaxPooling1D(pool_size=(8)))\\n\\n\\n\\nmodel.add(Conv1D(128, 5,padding='same',))                           #2\\nmodel.add(Activation('relu'))\\nmodel.add(Dropout(0.1))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(8))                                                 #3\\nmodel.add(Activation('softmax'))\\nopt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "#from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(layers.Conv1D(256, 8, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',input_shape=(X_train_expanded_dims.shape[1],1)))      #1\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',))                           #2\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))                                                 #3\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba88cec5-5048-42d1-be99-a479e463e36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from tensorflow import keras\\nmodel=models.Sequential()\\n###first layer\\nmodel.add(layers.Dense(100,input_shape=(X_train.shape[1],1)))\\nmodel.add(layers.Activation('relu'))\\nmodel.add(layers.Dropout(0.25))\\n###second layer\\nmodel.add(layers.Dense(200))\\nmodel.add(layers.Activation('relu'))\\n###third layer\\nmodel.add(layers.Dense(100))\\nmodel.add(layers.Activation('relu'))\\nmodel.add(layers.Flatten())\\n\\nmodel.add(layers.Dense(200))\\nmodel.add(layers.Activation('relu'))\\nmodel.add(layers.Flatten())\\n\\nmodel.add(layers.Dense(100))\\nmodel.add(layers.Activation('relu'))\\nmodel.add(layers.Dropout(0.25))\\nmodel.add(layers.Flatten())\\n\\n\\n\\n\\n\\n###final layer\\nmodel.add(layers.Dense(8))\\nmodel.add(layers.Activation('softmax'))\\nopt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from tensorflow import keras\n",
    "model=models.Sequential()\n",
    "###first layer\n",
    "model.add(layers.Dense(100,input_shape=(X_train.shape[1],1)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "###second layer\n",
    "model.add(layers.Dense(200))\n",
    "model.add(layers.Activation('relu'))\n",
    "###third layer\n",
    "model.add(layers.Dense(100))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(200))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(100))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###final layer\n",
    "model.add(layers.Dense(8))\n",
    "model.add(layers.Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\"\"\"\n",
    "#quick 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e58a41c5-2821-43e4-bcdf-86660f04ded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 229, 256)          2304      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 229, 256)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 229, 256)          524544    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 229, 256)          0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 229, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 28, 256)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 28, 128)           262272    \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 28, 128)           131200    \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 28, 128)           131200    \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 28, 128)           131200    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 28, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 3, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 3, 64)             65600     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 3, 64)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3, 100)            6500      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 3, 100)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3, 200)            20200     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 3, 200)            0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3, 100)            20100     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 3, 100)            0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3, 200)            20200     \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 3, 200)            0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3, 100)            20100     \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 3, 100)            0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 2408      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,337,828\n",
      "Trainable params: 1,337,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermecavalcantidesabarreto/.pyenv/versions/3.8.12/envs/silvertone/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv1D(256, 8, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "model.add(layers.Conv1D(128, 8, padding='same', activation = \"relu\"))\n",
    "#model.add(layers.Conv1D(128, 8, padding='same'))#\n",
    "#model.add(layers.Conv1D(128, 8, padding='same'))#\n",
    "#model.add(layers.Activation('relu'))#\n",
    "#model.add(layers.Conv1D(128, 8, padding='same'))#\n",
    "#model.add(layers.Activation('relu'))#\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model.add(layers.Conv1D(64, 8, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "#model.add(layers.Conv1D(64, 8, padding='same'))\n",
    "#model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dense(100,input_shape=(X_train.shape[1],1)))\n",
    "model.add(layers.Activation('relu'))\n",
    "###second layer\n",
    "model.add(layers.Dense(200))\n",
    "model.add(layers.Activation('relu'))\n",
    "###third layer\n",
    "model.add(layers.Dense(100))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(200))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(100))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(8, activation = \"softmax\"))\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.summary()\n",
    "###first layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19e5de5d-b018-4245-b186-302d61b0eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "26/26 [==============================] - 15s 541ms/step - loss: 2.0782 - accuracy: 0.1241 - val_loss: 2.0760 - val_accuracy: 0.1676\n",
      "Epoch 2/500\n",
      "26/26 [==============================] - 14s 533ms/step - loss: 2.0672 - accuracy: 0.1253 - val_loss: 2.0616 - val_accuracy: 0.1618\n",
      "Epoch 3/500\n",
      "26/26 [==============================] - 14s 535ms/step - loss: 2.0557 - accuracy: 0.1340 - val_loss: 2.0498 - val_accuracy: 0.2110\n",
      "Epoch 4/500\n",
      "26/26 [==============================] - 14s 534ms/step - loss: 2.0306 - accuracy: 0.1687 - val_loss: 2.0050 - val_accuracy: 0.1965\n",
      "Epoch 5/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 2.0133 - accuracy: 0.1985 - val_loss: 1.9830 - val_accuracy: 0.2457\n",
      "Epoch 6/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 1.9963 - accuracy: 0.1973 - val_loss: 2.0068 - val_accuracy: 0.2081\n",
      "Epoch 7/500\n",
      "26/26 [==============================] - 14s 553ms/step - loss: 1.9574 - accuracy: 0.2357 - val_loss: 1.9290 - val_accuracy: 0.2688\n",
      "Epoch 8/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 1.9595 - accuracy: 0.2444 - val_loss: 1.9255 - val_accuracy: 0.2803\n",
      "Epoch 9/500\n",
      "26/26 [==============================] - 14s 544ms/step - loss: 1.9295 - accuracy: 0.2519 - val_loss: 1.8838 - val_accuracy: 0.2977\n",
      "Epoch 10/500\n",
      "26/26 [==============================] - 14s 541ms/step - loss: 1.8891 - accuracy: 0.2643 - val_loss: 1.8700 - val_accuracy: 0.2543\n",
      "Epoch 11/500\n",
      "26/26 [==============================] - 14s 534ms/step - loss: 1.8740 - accuracy: 0.2643 - val_loss: 1.8922 - val_accuracy: 0.2601\n",
      "Epoch 12/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 1.8579 - accuracy: 0.2643 - val_loss: 1.8623 - val_accuracy: 0.2775\n",
      "Epoch 13/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 1.8620 - accuracy: 0.2829 - val_loss: 1.8042 - val_accuracy: 0.2803\n",
      "Epoch 14/500\n",
      "26/26 [==============================] - 14s 535ms/step - loss: 1.8300 - accuracy: 0.2940 - val_loss: 1.8360 - val_accuracy: 0.2717\n",
      "Epoch 15/500\n",
      "26/26 [==============================] - 14s 539ms/step - loss: 1.8098 - accuracy: 0.2928 - val_loss: 1.8185 - val_accuracy: 0.2717\n",
      "Epoch 16/500\n",
      "26/26 [==============================] - 14s 534ms/step - loss: 1.8099 - accuracy: 0.3002 - val_loss: 1.8120 - val_accuracy: 0.3035\n",
      "Epoch 17/500\n",
      "26/26 [==============================] - 17s 646ms/step - loss: 1.7865 - accuracy: 0.3213 - val_loss: 1.8037 - val_accuracy: 0.3121\n",
      "Epoch 18/500\n",
      "26/26 [==============================] - 15s 581ms/step - loss: 1.7768 - accuracy: 0.3164 - val_loss: 1.7597 - val_accuracy: 0.3150\n",
      "Epoch 19/500\n",
      "26/26 [==============================] - 16s 635ms/step - loss: 1.7957 - accuracy: 0.2953 - val_loss: 1.7959 - val_accuracy: 0.3121\n",
      "Epoch 20/500\n",
      "26/26 [==============================] - 16s 620ms/step - loss: 1.7509 - accuracy: 0.3102 - val_loss: 1.7795 - val_accuracy: 0.2890\n",
      "Epoch 21/500\n",
      "26/26 [==============================] - 18s 692ms/step - loss: 1.7456 - accuracy: 0.3164 - val_loss: 1.7964 - val_accuracy: 0.3150\n",
      "Epoch 22/500\n",
      "26/26 [==============================] - 21s 800ms/step - loss: 1.7460 - accuracy: 0.3263 - val_loss: 1.8033 - val_accuracy: 0.3121\n",
      "Epoch 23/500\n",
      "26/26 [==============================] - 21s 797ms/step - loss: 1.7299 - accuracy: 0.3275 - val_loss: 1.7752 - val_accuracy: 0.3179\n",
      "Epoch 24/500\n",
      "26/26 [==============================] - 22s 856ms/step - loss: 1.7205 - accuracy: 0.3362 - val_loss: 1.7338 - val_accuracy: 0.3353\n",
      "Epoch 25/500\n",
      "26/26 [==============================] - 22s 844ms/step - loss: 1.7185 - accuracy: 0.3189 - val_loss: 1.7380 - val_accuracy: 0.3121\n",
      "Epoch 26/500\n",
      "26/26 [==============================] - 16s 602ms/step - loss: 1.7022 - accuracy: 0.3511 - val_loss: 1.7046 - val_accuracy: 0.3179\n",
      "Epoch 27/500\n",
      "26/26 [==============================] - 14s 538ms/step - loss: 1.6821 - accuracy: 0.3486 - val_loss: 1.7332 - val_accuracy: 0.3468\n",
      "Epoch 28/500\n",
      "26/26 [==============================] - 14s 534ms/step - loss: 1.6500 - accuracy: 0.3623 - val_loss: 1.7487 - val_accuracy: 0.3266\n",
      "Epoch 29/500\n",
      "26/26 [==============================] - 14s 531ms/step - loss: 1.6701 - accuracy: 0.3375 - val_loss: 1.7345 - val_accuracy: 0.3266\n",
      "Epoch 30/500\n",
      "26/26 [==============================] - 14s 534ms/step - loss: 1.6450 - accuracy: 0.3536 - val_loss: 1.7077 - val_accuracy: 0.3497\n",
      "Epoch 31/500\n",
      "26/26 [==============================] - 14s 534ms/step - loss: 1.6161 - accuracy: 0.3883 - val_loss: 1.6866 - val_accuracy: 0.3468\n",
      "Epoch 32/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 1.6238 - accuracy: 0.3623 - val_loss: 1.6989 - val_accuracy: 0.3613\n",
      "Epoch 33/500\n",
      "26/26 [==============================] - 14s 531ms/step - loss: 1.5983 - accuracy: 0.3821 - val_loss: 1.7252 - val_accuracy: 0.3266\n",
      "Epoch 34/500\n",
      "26/26 [==============================] - 14s 536ms/step - loss: 1.6099 - accuracy: 0.3623 - val_loss: 1.7036 - val_accuracy: 0.3815\n",
      "Epoch 35/500\n",
      "26/26 [==============================] - 15s 586ms/step - loss: 1.5821 - accuracy: 0.3970 - val_loss: 1.6876 - val_accuracy: 0.3439\n",
      "Epoch 36/500\n",
      "26/26 [==============================] - 329s 13s/step - loss: 1.5880 - accuracy: 0.3797 - val_loss: 1.6975 - val_accuracy: 0.3439\n",
      "Epoch 37/500\n",
      "26/26 [==============================] - 14s 535ms/step - loss: 1.5764 - accuracy: 0.3759 - val_loss: 1.7483 - val_accuracy: 0.3353\n",
      "Epoch 38/500\n",
      "26/26 [==============================] - 14s 530ms/step - loss: 1.5861 - accuracy: 0.3834 - val_loss: 1.6893 - val_accuracy: 0.3526\n",
      "Epoch 39/500\n",
      "26/26 [==============================] - 14s 530ms/step - loss: 1.5586 - accuracy: 0.3859 - val_loss: 1.6904 - val_accuracy: 0.3353\n",
      "Epoch 40/500\n",
      "26/26 [==============================] - 14s 532ms/step - loss: 1.5405 - accuracy: 0.3945 - val_loss: 1.6697 - val_accuracy: 0.3815\n",
      "Epoch 41/500\n",
      "26/26 [==============================] - 14s 533ms/step - loss: 1.5412 - accuracy: 0.4144 - val_loss: 1.6805 - val_accuracy: 0.3786\n",
      "Epoch 42/500\n",
      "26/26 [==============================] - 14s 537ms/step - loss: 1.5247 - accuracy: 0.4020 - val_loss: 1.7088 - val_accuracy: 0.3410\n",
      "Epoch 43/500\n",
      "26/26 [==============================] - 14s 535ms/step - loss: 1.4971 - accuracy: 0.3995 - val_loss: 1.6804 - val_accuracy: 0.3237\n",
      "Epoch 44/500\n",
      "26/26 [==============================] - 14s 528ms/step - loss: 1.5114 - accuracy: 0.4082 - val_loss: 1.7121 - val_accuracy: 0.3410\n",
      "Epoch 45/500\n",
      "26/26 [==============================] - 14s 539ms/step - loss: 1.4817 - accuracy: 0.4194 - val_loss: 1.6987 - val_accuracy: 0.3410\n",
      "Epoch 46/500\n",
      "26/26 [==============================] - 15s 560ms/step - loss: 1.4509 - accuracy: 0.4231 - val_loss: 1.6650 - val_accuracy: 0.3353\n",
      "Epoch 47/500\n",
      "26/26 [==============================] - 1912s 76s/step - loss: 1.4347 - accuracy: 0.4529 - val_loss: 1.6586 - val_accuracy: 0.3439\n",
      "Epoch 48/500\n",
      "26/26 [==============================] - 3000s 77s/step - loss: 1.4479 - accuracy: 0.4491 - val_loss: 1.6783 - val_accuracy: 0.3382\n",
      "Epoch 49/500\n",
      "26/26 [==============================] - 1958s 78s/step - loss: 1.4457 - accuracy: 0.4330 - val_loss: 1.6599 - val_accuracy: 0.3497\n",
      "Epoch 50/500\n",
      "26/26 [==============================] - 2889s 75s/step - loss: 1.4227 - accuracy: 0.4516 - val_loss: 1.6651 - val_accuracy: 0.3555\n",
      "Epoch 51/500\n",
      "26/26 [==============================] - 2917s 117s/step - loss: 1.4654 - accuracy: 0.4392 - val_loss: 1.6744 - val_accuracy: 0.3584\n",
      "Epoch 52/500\n",
      "26/26 [==============================] - 2115s 85s/step - loss: 1.4046 - accuracy: 0.4516 - val_loss: 1.6756 - val_accuracy: 0.3468\n",
      "Epoch 53/500\n",
      "26/26 [==============================] - 2065s 83s/step - loss: 1.3783 - accuracy: 0.4615 - val_loss: 1.6637 - val_accuracy: 0.3526\n",
      "Epoch 54/500\n",
      "26/26 [==============================] - 2863s 77s/step - loss: 1.3638 - accuracy: 0.5012 - val_loss: 1.6842 - val_accuracy: 0.3266\n",
      "Epoch 55/500\n",
      "26/26 [==============================] - 2060s 82s/step - loss: 1.3698 - accuracy: 0.4690 - val_loss: 1.6643 - val_accuracy: 0.3353\n",
      "Epoch 56/500\n",
      "26/26 [==============================] - 2943s 118s/step - loss: 1.3147 - accuracy: 0.4913 - val_loss: 1.6925 - val_accuracy: 0.3382\n",
      "Epoch 57/500\n",
      "26/26 [==============================] - 2121s 85s/step - loss: 1.3312 - accuracy: 0.4764 - val_loss: 1.6730 - val_accuracy: 0.3642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a1af5b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.fit(X_train_expanded_dims, y_train_array, validation_split=0.3, epochs=500, batch_size=32, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f4a0fb-f2dc-4c36-8d03-a310c8a95684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#model.compile(optimizer=optimiser,loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec3a5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85c2329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "#model.fit(X_train_expanded_dims, y_train_array, validation_split=0.3, epochs=500, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a263ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 75ms/step - loss: 1.8147 - accuracy: 0.3125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8147146701812744, 0.3125]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_expanded_dims, y_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5227bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
