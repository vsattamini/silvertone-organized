{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32d4279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:57:25.929443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-22 15:57:25.929525: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "#to make plots look nice\n",
    "from itertools import cycle\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879617d2",
   "metadata": {},
   "source": [
    "# - Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46375b7",
   "metadata": {},
   "source": [
    "## retrieving audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6174332",
   "metadata": {},
   "source": [
    "- this is my local repository, to change it with retrieving code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e99b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob(\"data/Emotion_Dataset/*/*.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9655d",
   "metadata": {},
   "source": [
    "## Treating audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3d37e",
   "metadata": {},
   "source": [
    "- loading audio into a vector. Using stft to convert it into a spectrogram. Saving all spectrograms to make them our X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3a2bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:57:35.209214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-22 15:57:35.209313: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-22 15:57:35.209346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-23E8V41): /proc/driver/nvidia/version does not exist\n",
      "2022-11-22 15:57:35.210779: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "spectrograms = []\n",
    "for item in audio_files:\n",
    "    X, sr = librosa.load(item) #vectorizing\n",
    "    X_trim = librosa.effects.trim(X,top_db=35) #trimming data\n",
    "    \n",
    "    #padding\n",
    "    if X_trim[0].shape[0] >= 48000:\n",
    "        X_final = X_trim[0][:48000]\n",
    "        X_final = tf.convert_to_tensor(X_final).numpy()\n",
    "    else:\n",
    "        zero_padding = tf.zeros([48000]-tf.shape(X_trim[0]),dtype=tf.float32)\n",
    "        X_final = tf.concat([X_trim[0],zero_padding],0).numpy()\n",
    "    \n",
    "    fourrier = librosa.stft(X_final) #fourrier\n",
    "    spectrogram = tf.abs(fourrier)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    spectrograms.append(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a444ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in spectrograms:\n",
    "    X.append(x.numpy())\n",
    "X= np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756de098",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635bb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd6f9a",
   "metadata": {},
   "source": [
    "### retrieving audio name and getting out just the emotion label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7e2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[]\n",
    "for audio in audio_files:\n",
    "    ids.append(audio[30:-4])\n",
    "\n",
    "new_df = pd.DataFrame(ids)\n",
    "new_df.rename(columns={0: \"titulo\"},inplace=True)\n",
    "new_df['emotion'] = new_df.apply(lambda x: x[0].split(\"-\")[2], axis=1)\n",
    "y = new_df['emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44633fd",
   "metadata": {},
   "source": [
    "### One hot encoding y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ac9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "a =encoder.fit_transform(new_df[['emotion']])\n",
    "enc_df = pd.DataFrame(a.toarray())\n",
    "new_df = new_df.join(enc_df)\n",
    "y = new_df.drop(columns=['titulo','emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be28bea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a964bcf",
   "metadata": {},
   "source": [
    "## split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131dc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df00f06",
   "metadata": {},
   "source": [
    "# - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d5847",
   "metadata": {},
   "source": [
    "- Since we are dealing with spectograms, the best idea was to use a CNN since we are dealing with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf621caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten,MaxPool2D,Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa525406",
   "metadata": {},
   "source": [
    "## model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8a96751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3,3), activation='relu', input_shape=(1025, 94, 1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66228d",
   "metadata": {},
   "source": [
    "## model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f66f141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a3933d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 1023, 92, 16)      160       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 511, 46, 16)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 509, 44, 16)       2320      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 254, 22, 16)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 89408)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                1430544   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433,160\n",
      "Trainable params: 1,433,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb16a25",
   "metadata": {},
   "source": [
    "## model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f54d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,validation_split=0.3,epochs=50,batch_size=32,callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8170e1d",
   "metadata": {},
   "source": [
    "## model evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33790698",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31543268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
